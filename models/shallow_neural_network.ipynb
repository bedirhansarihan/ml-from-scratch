{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2024-08-22T08:57:39.296388Z",
          "start_time": "2024-08-22T08:57:39.291421Z"
        },
        "id": "initial_id"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T08:58:14.203919Z",
          "start_time": "2024-08-22T08:58:14.198229Z"
        },
        "id": "a0aa11b470206f33"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(Z: np.array):\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    return A\n",
        "\n",
        "\n",
        "def relu(X: np.array):\n",
        "    A = np.maximum(0, X)\n",
        "    return A\n"
      ],
      "id": "a0aa11b470206f33",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "4ebb883987bbf705"
      },
      "cell_type": "markdown",
      "source": [
        "**Note:** The dataset used in this implementation is sourced from the Deep Learning Specialization.\n"
      ],
      "id": "4ebb883987bbf705"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T08:58:19.410784Z",
          "start_time": "2024-08-22T08:58:19.404311Z"
        },
        "id": "c034524f18778d85"
      },
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    train_dataset = h5py.File('/content/train_catvnoncat.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('/content/test_catvnoncat.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "\n",
        "    y_train = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    y_test = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "    # Reshape the training and test examples\n",
        "    train_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "    test_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
        "\n",
        "    # Standardize data to have feature values between 0 and 1.\n",
        "    X_train = train_x_flatten/255.\n",
        "    X_test = test_x_flatten/255.\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, classes\n",
        "\n"
      ],
      "id": "c034524f18778d85",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T08:58:19.621851Z",
          "start_time": "2024-08-22T08:58:19.591134Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb87a26ef890b577",
        "outputId": "b242f98f-ac30-410f-a633-1ba734394928"
      },
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test, _ = load_data()\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n"
      ],
      "id": "bb87a26ef890b577",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12288, 209)\n",
            "(1, 209)\n",
            "(12288, 50)\n",
            "(1, 50)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7bdb6b06adbecf49"
      },
      "cell_type": "markdown",
      "source": [
        "# Two Hidden Layer Neural Network Implementation\n",
        "\n",
        "In this notebook, we will implement a two-hidden-layer neural network for binary classification problem from scratch using vectorized operations. The steps we will follow include:\n",
        "\n",
        "1. **Parameter Initialization**: Initialize the parameters (weights and biases) for the neural network.\n",
        "2. **Forward Propagation**: Implement the forward propagation step to compute the activations at each layer.\n",
        "3. **Cost Function Computation**: Compute the cost function to measure the model's performance.\n",
        "4. **Backward Propagation**: Implement the backward propagation step to compute gradients with respect to the parameters.\n",
        "5. **Model**: Integrate all functions into a model that can train the neural network using gradient descent.\n",
        "6. **Predict**: Use the trained model to make predictions on new data."
      ],
      "id": "7bdb6b06adbecf49"
    },
    {
      "metadata": {
        "id": "7a9be62330fbfd70"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Parameter Initialization\n",
        "\n",
        "We initialize the parameters for the neural network. The parameters include weights (`W`) and biases (`b`) for each layer. For a neural network with `L` layers (including the input layer, two hidden layers, and the output layer), the parameters will be stored in dictionaries.\n",
        "\n",
        "### Function: `parameter_initialization(layer_dims)`\n",
        "- **Input**:\n",
        "  - `layer_dims` (a list containing the dimensions of each layer in the network, e.g., `[n_x, n_h1, n_h2, n_y]` where `n_x` is the number of input features, `n_h1` and `n_h2` are the number of units in the first and second hidden layers, and `n_y` is the number of output units).\n",
        "  \n",
        "- **Output**:\n",
        "  - `parameters` (a dictionary containing initialized parameters `W` and `b` for each layer).\n",
        "  \n",
        "- **Dimension Constraints**:\n",
        "  - `W[l].shape = (layer_dims[l], layer_dims[l-1])`\n",
        "  - `b[l].shape = (layer_dims[l], 1)`\n",
        "    -"
      ],
      "id": "7a9be62330fbfd70"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T08:58:22.317290Z",
          "start_time": "2024-08-22T08:58:22.312580Z"
        },
        "id": "9410ce1450df64d8"
      },
      "cell_type": "code",
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "    parameters = dict()\n",
        "\n",
        "    for l in range(1, len(layer_dims)):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "id": "9410ce1450df64d8",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T08:58:22.824244Z",
          "start_time": "2024-08-22T08:58:22.796481Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e1f7b01c7145be5",
        "outputId": "36ff54af-afae-4f9c-c43b-82df2d448986"
      },
      "cell_type": "code",
      "source": [
        "# initalize_parameters_test\n",
        "X_train, _, _, _, _ = load_data()\n",
        "m = X_train.shape[1]\n",
        "layer_dims = (m, 2, 1)\n",
        "\n",
        "parameters = initialize_parameters(layer_dims)\n",
        "print(\"shape of W1: \",parameters['W1'].shape)\n",
        "print(\"shape of b1: \",parameters['b1'].shape)\n",
        "print(\"shape of W2: \",parameters['W2'].shape)\n",
        "print(\"shape of b2: \",parameters['b2'].shape)"
      ],
      "id": "2e1f7b01c7145be5",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of W1:  (2, 209)\n",
            "shape of b1:  (2, 1)\n",
            "shape of W2:  (1, 2)\n",
            "shape of b2:  (1, 1)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Compute Cost\n",
        "\n",
        "The `compute_cost` function calculates the cost of the predictions made by the model using binary cross-entropy loss.\n",
        "\n",
        "### Function: `compute_cost(A_L, Y)`\n",
        "- **Input**:\n",
        "  - `A_L` (activation of the final layer, shape: `(n_L, m)`)\n",
        "  - `Y` (true labels, shape: `(n_L, m)`)\n",
        "- **Output**:\n",
        "  - `cost` (scalar value representing the cost)"
      ],
      "metadata": {
        "id": "D-icW_wGAcUr"
      },
      "id": "D-icW_wGAcUr"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(A_L, Y):\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute the binary cross-entropy cost\n",
        "    cost = - (1 / m) * np.sum(Y * np.log(A_L) + (1 - Y) * np.log(1 - A_L))\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "uO_JncOUAbZk"
      },
      "id": "uO_JncOUAbZk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute_cost_test\n",
        "\n",
        "A_L = np.array([[0.99, 0.1, 0.99]])\n",
        "Y = np.array([[1, 0, 1]])\n",
        "\n",
        "cost = compute_cost(A_L, Y)\n",
        "print(f\"Cost: {cost}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvpj61GlKgdT",
        "outputId": "7a6437a6-8c22-4da2-f824-75453d8c326e"
      },
      "id": "Dvpj61GlKgdT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost: 0.0418203957882764\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "efd0f70c6efab29c"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Forward Propagation\n",
        "\n",
        "The forward propagation step computes the activations for each layer, starting from the input layer to the output layer. The activation functions used in the layers are ReLU or sigmoid.\n",
        "\n",
        "### Function: `forward_propagation(X, parameters)`\n",
        "- **Input**:\n",
        "  - `X` (input data of shape `(n_x, m)`)\n",
        "  - `parameters` (a dictionary containing the parameters `W` and `b` for each layer)\n",
        "- **Output**:\n",
        "  - `A_L` (activation of the final layer, shape: `(1, m)`)\n",
        "  - `caches` (list of caches containing intermediate values for use in backpropagation)\n",
        "\n",
        "  #### Subroutine: `linear_activation_forward(A_prev, W, b, activation_type='relu')`\n",
        "  - **Input**:\n",
        "    - `A_prev` (shape: `(n_{l-1}, m)`)\n",
        "    - `W` (shape: `(n_l, n_{l-1})`)\n",
        "    - `b` (shape: `(n_l, 1)`)\n",
        "    - `activation_type` ('relu' or 'sigmoid')\n",
        "  - **Output**:\n",
        "    - `AL` (shape: `(n_l, m)`)\n",
        "    - `cache` (values for backpropagation)\n",
        "\n",
        "  #### Subroutine: `linear_forward(A, W, b)`\n",
        "  - **Input**:\n",
        "    - `A` (shape: `(n_{l-1}, m)`)\n",
        "    - `W` (shape: `(n_l, n_{l-1})`)\n",
        "    - `b` (shape: `(n_l, 1)`)\n",
        "  - **Output**:\n",
        "    - `Z` (shape: `(n_l, m)`)\n",
        "    - `cache` (values for backpropagation)\n",
        "\n",
        "These subroutines are part of the `forward_propagation` function.\n"
      ],
      "id": "efd0f70c6efab29c"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T08:58:23.926397Z",
          "start_time": "2024-08-22T08:58:23.921677Z"
        },
        "id": "fbcab4bf1822d0d7"
      },
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "\n",
        "    Z = np.dot(W, A) + b\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache"
      ],
      "id": "fbcab4bf1822d0d7",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T08:58:24.730623Z",
          "start_time": "2024-08-22T08:58:24.724733Z"
        },
        "id": "4f4670f039073930"
      },
      "cell_type": "code",
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation_type='relu'):\n",
        "\n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "    AL = None\n",
        "\n",
        "    if activation_type == 'relu':\n",
        "        AL = relu(Z)\n",
        "    elif activation_type == 'sigmoid':\n",
        "        AL = sigmoid(Z)\n",
        "    activation_cache = Z\n",
        "\n",
        "    cache = (linear_cache, activation_cache)\n",
        "    return AL, cache\n",
        "\n"
      ],
      "id": "4f4670f039073930",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T08:59:42.725578Z",
          "start_time": "2024-08-22T08:59:42.718369Z"
        },
        "id": "7c1c84ff8cf67482"
      },
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    caches = []\n",
        "    layers_len = len(parameters) // 2\n",
        "\n",
        "    ###### HIDDEN LAYERS ######\n",
        "    A_prev = X\n",
        "    for l in range(1, layers_len):\n",
        "\n",
        "        W_l = parameters['W' + str(l)]\n",
        "        b_l = parameters['b' + str(l)]\n",
        "\n",
        "        A_L, cache = linear_activation_forward(A_prev, W_l, b_l, activation_type='relu')\n",
        "        caches.append(cache)\n",
        "        A_prev = A_L\n",
        "\n",
        "    ###### OUTPUT LAYER ######\n",
        "    W_l = parameters['W' + str(layers_len)]\n",
        "    b_l = parameters['b' + str(layers_len)]\n",
        "    A_L, cache = linear_activation_forward(A_prev, W_l, b_l, activation_type=\"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    return A_L, caches\n",
        "\n"
      ],
      "id": "7c1c84ff8cf67482",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-22T09:02:13.276674Z",
          "start_time": "2024-08-22T09:02:13.240491Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee18698209f1274a",
        "outputId": "209cb07d-ba02-4481-8fb7-8facca20bf72"
      },
      "cell_type": "code",
      "source": [
        "# forward_propagation_test\n",
        "X_train, _, _, _, _ = load_data()\n",
        "layer_dims = (X_train.shape[0], 2, 1)\n",
        "\n",
        "initial_params = initialize_parameters(layer_dims)\n",
        "A_L, caches = forward_propagation(X_train, parameters=initial_params)\n",
        "A_L.shape"
      ],
      "id": "ee18698209f1274a",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 209)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Backpropagation\n",
        "\n",
        "The backpropagation step computes the gradients for each layer, starting from the output layer and moving backward through the network. It uses cached values from forward propagation to compute gradients for weights, biases, and activations.\n",
        "\n",
        "### Function: `backpropagation(A_L, Y, caches)`\n",
        "- **Input**:\n",
        "  - `A_L` (activation of the final layer, shape: `(1, m)`)\n",
        "  - `Y` (true labels, shape: `(n_L, m)`)\n",
        "  - `caches` (list of caches from forward propagation, each cache contains `(linear_cache, activation_cache)`)\n",
        "- **Output**:\n",
        "  - `gradients` (dictionary containing gradients for each layer: `dA_prev`, `dW`, and `db`)\n",
        "\n",
        "  #### Subroutine: `backward_step(dA, cache, activation_type='relu')`\n",
        "  - **Input**:\n",
        "    - `dA` (gradient of the activation, shape: `(n_l, m)`)\n",
        "    - `cache` (tuple of `(linear_cache, activation_cache)`, where `linear_cache` contains `(A_prev, W, b)` and `activation_cache` contains `Z`)\n",
        "    - `activation_type` ('relu' or 'sigmoid')\n",
        "  - **Output**:\n",
        "    - `dA_prev` (gradient of the previous layer, shape: `(n_{l-1}, m)`)\n",
        "    - `dW` (gradient of the weights, shape: `(n_l, n_{l-1})`)\n",
        "    - `db` (gradient of the bias, shape: `(n_l, 1)`)\n",
        "\n",
        "  This subroutine computes the gradient of the loss function with respect to the activation, weights, and bias for a given layer.\n",
        "\n",
        "These subroutines are used within the `backpropagation` function to calculate the gradients necessary for updating the network parameters during training.\n"
      ],
      "metadata": {
        "id": "ynUwZomU_YB4"
      },
      "id": "ynUwZomU_YB4"
    },
    {
      "cell_type": "code",
      "source": [
        "# cache = (linear_cache, activation_cache) ((A_prev, W, b), Z)\n",
        "def backward_step(dA, cache, activation_type='relu'):\n",
        "  linear_cache, activation_cache = cache\n",
        "  A_prev, W, b  = linear_cache\n",
        "  Z = activation_cache\n",
        "  m = A_prev.shape[1]\n",
        "\n",
        "  if activation_type == 'relu':\n",
        "      dZ = None # relu_gradient(Z):\n",
        "  elif activation_type == 'sigmoid':\n",
        "      dZ = None #sigmoid_gradient(Z):\n",
        "\n",
        "  dA_prev = np.dot(W.T, dZ)\n",
        "  dW = np.dot(dZ, A_prev.T) / m\n",
        "  db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "\n",
        "  return dA_prev, dW, db\n",
        "\n"
      ],
      "metadata": {
        "id": "KCeBYAgeqDLM"
      },
      "id": "KCeBYAgeqDLM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropagation(A_L, Y, caches):\n",
        "  gradients = {}\n",
        "  m = A_L.shape[1]\n",
        "  dA_L = - (np.divide(Y, A_L) - np.divide(1 - Y, 1 - A_L))\n",
        "  L = len(caches)\n",
        "\n",
        "\n",
        "  # Lth Layer\n",
        "  dA_prev_L, dW_L, db_L = backward_step(dA_L, caches[-1], activation_type='sigmoid')\n",
        "\n",
        "  gradients['dA_prev'+str(L)] = dA_prev_L\n",
        "  gradients['dW'+str(L)] = dW_L\n",
        "  gradients['db'+str(L)] = db_L\n",
        "\n",
        "  for l in range(L-1, -1, 0):\n",
        "\n",
        "      dA_prev, dW, dB = backward_step(dA_prev_L, caches[l], activation_type='relu')\n",
        "\n",
        "      gradients['dA_prev'+str(L)] = dA_prev\n",
        "      gradients['dW'+str(L)] = dW\n",
        "      gradients['db'+str(L)] = db\n",
        "\n",
        "\n",
        "  return gradients\n"
      ],
      "metadata": {
        "id": "UdMX38cqsC2g"
      },
      "id": "UdMX38cqsC2g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "  L = len(parameters) // 2\n",
        "  params = deepcopy(parameters)\n",
        "\n",
        "  for l in range(1, L):\n",
        "      params['W'+str(l)] = params['W'+str(l)] - learning_rate * grads['dW'+str(l)]\n",
        "      params['b'+str(l)] = params['b'+str(l)] - learning_rate * grads['db'+str(l)]\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "81Nk-8_j52tz"
      },
      "id": "81Nk-8_j52tz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MxVHduhO6tTS"
      },
      "id": "MxVHduhO6tTS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}